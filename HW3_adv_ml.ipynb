{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_adv_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5jFXXhrRrQD",
        "colab_type": "text"
      },
      "source": [
        "#### Raquel Senior \n",
        "\n",
        "# Homework 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfiVlnEFbHMb",
        "colab_type": "text"
      },
      "source": [
        "1)  Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine.\n",
        "\n",
        "2) Preprocess your data such that each document in the data is represented as a sequence of equal length.\n",
        "\n",
        "3)  Use the data to fit separate models to each of the following architectures:\n",
        "\n",
        "A. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)\n",
        "\n",
        "B. A model using an Embedding layer with Conv1d Layers\n",
        "\n",
        "C. A model using an Embedding layer with one sequential layer (LSTM or GRU)\n",
        "\n",
        "D. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)\n",
        "\n",
        "E. A model using an Embedding layer with bidirectional sequential layers\n",
        "\n",
        "F. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!).\n",
        "\n",
        "4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yudj4kvdCQs",
        "colab_type": "code",
        "outputId": "75099cf3-d1f6-41d5-db18-06cad85916c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df=pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSGsvHtb-yam",
        "colab_type": "code",
        "outputId": "ad6731c1-1905-465a-c24c-259cb3045283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df['category_id'] = df['category'].factorize()[0]\n",
        "df.groupby('category').category_id.count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "business         510\n",
              "entertainment    386\n",
              "politics         417\n",
              "sport            511\n",
              "tech             401\n",
              "Name: category_id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROC-MZxSIqG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "7273321c-d2a3-4e94-f649-8aa063b0e5e4"
      },
      "source": [
        "# Visualize \n",
        "\n",
        "df.groupby('category').category_id.count().plot.bar(ylim=0)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f893b92aef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT0bXHzohG0k",
        "colab_type": "text"
      },
      "source": [
        "This dataset includes BBC news articles about tech, business, sport, entertainment and politics. Most articles are about sports followed by business, politics, tech, and entertainment in that order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeFngFq6fCgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess data \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "texts = df['text']\n",
        "\n",
        "categories = df['category']\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y = ohe.fit_transform(np.asarray(categories).reshape(-1,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aive7FXbfE9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff717a23-ad46-48ff-fc41-df10062ca50f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "max_words = 10000\n",
        "\n",
        "train_len = 1500\n",
        "\n",
        "val_len = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "tok_text = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(tok_text, maxlen=max_len)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data = data[indices]\n",
        "labels = y[indices]\n",
        "\n",
        "X_train = data[:train_len]\n",
        "y_train = labels[:train_len]\n",
        "\n",
        "X_val = data[train_len: train_len + val_len]\n",
        "y_val = labels[train_len: train_len + val_len]\n",
        "\n",
        "X_test = data[train_len + val_len:]\n",
        "y_test = labels[train_len + val_len:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEX9Rungx7tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding, Flatten, Dense\n",
        "from keras.layers import SimpleRNN, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoKyQyDaM-bN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "0ec58db9-df18-4f7b-b569-c6b576d2f80d"
      },
      "source": [
        "# A\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words,250,input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(5, activation= 'softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc']) \n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5773 - acc: 0.2947 - val_loss: 1.5125 - val_acc: 0.4300\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9023 - acc: 0.9653 - val_loss: 1.2644 - val_acc: 0.5560\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2559 - acc: 1.0000 - val_loss: 1.0185 - val_acc: 0.6380\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0371 - acc: 1.0000 - val_loss: 0.8588 - val_acc: 0.6800\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.7829 - val_acc: 0.7160\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.7705 - val_acc: 0.7320\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 6.4224e-04 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.7360\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 3.8284e-04 - acc: 1.0000 - val_loss: 0.7503 - val_acc: 0.7360\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.7216e-04 - acc: 1.0000 - val_loss: 0.7376 - val_acc: 0.7380\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 2.1784e-04 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.7500\n",
            "500/500 [==============================] - 0s 253us/step\n",
            "Test score: 0.7263191640377045\n",
            "Test accuracy: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUJ_26K8QABF",
        "colab_type": "code",
        "outputId": "0280959e-062c-48e0-ea15-47ae8607563d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# B Cov1d\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 250, input_length=max_len)) \n",
        "model.add(layers.Conv1D(128, 7, activation='relu')) \n",
        "model.add(layers.MaxPooling1D(5)) \n",
        "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(5))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 5s 4ms/step - loss: 4.1694 - acc: 0.1893 - val_loss: 3.7471 - val_acc: 0.1620\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 4.0810 - acc: 0.1893 - val_loss: 3.6774 - val_acc: 0.1620\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 4.0127 - acc: 0.1893 - val_loss: 3.6153 - val_acc: 0.1620\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.9465 - acc: 0.1893 - val_loss: 3.5516 - val_acc: 0.1620\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.8744 - acc: 0.1893 - val_loss: 3.4779 - val_acc: 0.1620\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.7844 - acc: 0.1893 - val_loss: 3.3802 - val_acc: 0.1620\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.6555 - acc: 0.1893 - val_loss: 3.2250 - val_acc: 0.1620\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.4516 - acc: 0.1893 - val_loss: 2.9552 - val_acc: 0.1620\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 3.1472 - acc: 0.1893 - val_loss: 2.8045 - val_acc: 0.1620\n",
            "500/500 [==============================] - 0s 775us/step\n",
            "Test score: 2.804548501968384\n",
            "Test accuracy: 0.16200000047683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8KoR32dQAUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "0dd2b655-e086-4285-a300-30baf7011d51"
      },
      "source": [
        "# C One sequential \n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250, input_length=max_len))\n",
        "model.add(LSTM(250))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(5, activation= 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 14s 9ms/step - loss: 1.6047 - acc: 0.2487 - val_loss: 1.5913 - val_acc: 0.3340\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.5433 - acc: 0.3667 - val_loss: 1.4401 - val_acc: 0.2760\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.2742 - acc: 0.4327 - val_loss: 1.1288 - val_acc: 0.5400\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.9450 - acc: 0.5947 - val_loss: 0.9928 - val_acc: 0.7240\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.6903 - acc: 0.7640 - val_loss: 1.0697 - val_acc: 0.6420\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.7996 - acc: 0.7567 - val_loss: 0.8132 - val_acc: 0.6960\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.4675 - acc: 0.9027 - val_loss: 0.6889 - val_acc: 0.7760\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2548 - acc: 0.9460 - val_loss: 0.5368 - val_acc: 0.8120\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1825 - acc: 0.9427 - val_loss: 0.5115 - val_acc: 0.8400\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1247 - acc: 0.9720 - val_loss: 0.4529 - val_acc: 0.8580\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0578 - acc: 0.9967 - val_loss: 0.4204 - val_acc: 0.8680\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0214 - acc: 0.9973 - val_loss: 0.5059 - val_acc: 0.8840\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0113 - acc: 0.9960 - val_loss: 0.5769 - val_acc: 0.8700\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0074 - acc: 0.9993 - val_loss: 0.5684 - val_acc: 0.8680\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.6107 - val_acc: 0.8480\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.5226 - val_acc: 0.8620\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5196 - val_acc: 0.8680\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.8700\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 7.7936e-04 - acc: 1.0000 - val_loss: 0.5492 - val_acc: 0.8780\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 5.6158e-04 - acc: 1.0000 - val_loss: 0.5639 - val_acc: 0.8760\n",
            "500/500 [==============================] - 1s 2ms/step\n",
            "Test score: 0.5638555884361267\n",
            "Test accuracy: 0.8759999871253967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BehyEoJdrKkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "f0549f47-2b1a-492e-8e09-40d3bc6b57cf"
      },
      "source": [
        "# GRU-one sequential\n",
        "from keras.layers import GRU, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250, input_length=max_len))\n",
        "model.add(GRU(250))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(5, activation= 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.6063 - acc: 0.2540 - val_loss: 1.6002 - val_acc: 0.2780\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 1.5749 - acc: 0.3913 - val_loss: 1.5760 - val_acc: 0.3400\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 1.4812 - acc: 0.4740 - val_loss: 1.4862 - val_acc: 0.4440\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 1.1460 - acc: 0.6727 - val_loss: 1.3833 - val_acc: 0.5380\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.7675 - acc: 0.7427 - val_loss: 0.9768 - val_acc: 0.6220\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.4053 - acc: 0.9227 - val_loss: 0.9064 - val_acc: 0.7160\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1995 - acc: 0.9633 - val_loss: 0.8753 - val_acc: 0.7380\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0777 - acc: 0.9773 - val_loss: 1.0897 - val_acc: 0.7140\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0585 - acc: 0.9853 - val_loss: 1.1184 - val_acc: 0.6760\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0381 - acc: 0.9953 - val_loss: 1.0286 - val_acc: 0.7060\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0252 - acc: 0.9973 - val_loss: 1.0379 - val_acc: 0.7260\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 1.0683 - val_acc: 0.7280\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 1.0830 - val_acc: 0.7320\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 1.1028 - val_acc: 0.7360\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 1.1167 - val_acc: 0.7300\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.1254 - val_acc: 0.7340\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.1341 - val_acc: 0.7320\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 9.2705e-04 - acc: 1.0000 - val_loss: 1.1405 - val_acc: 0.7300\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 7.7399e-04 - acc: 1.0000 - val_loss: 1.1451 - val_acc: 0.7320\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 6.6553e-04 - acc: 1.0000 - val_loss: 1.1486 - val_acc: 0.7320\n",
            "500/500 [==============================] - 2s 3ms/step\n",
            "Test score: 1.1486112475395203\n",
            "Test accuracy: 0.7319999933242798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJY--L-IsTI4",
        "colab_type": "text"
      },
      "source": [
        "The LSTM model with a one sequential layer has a higher accuracy score than its GRU counterpart. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swodex29QRuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "f20ecf7e-8b0e-46da-f105-801e2b6e5124"
      },
      "source": [
        "# D Stacked \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250,input_length=max_len))\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 16s 11ms/step - loss: 1.6034 - acc: 0.2233 - val_loss: 1.5852 - val_acc: 0.2280\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.4750 - acc: 0.3900 - val_loss: 1.2575 - val_acc: 0.4800\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.0786 - acc: 0.5680 - val_loss: 1.0558 - val_acc: 0.6180\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.8870 - acc: 0.7147 - val_loss: 0.9103 - val_acc: 0.7020\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.6647 - acc: 0.8320 - val_loss: 0.7583 - val_acc: 0.7580\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.4549 - acc: 0.9240 - val_loss: 0.7688 - val_acc: 0.7560\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.3111 - acc: 0.9633 - val_loss: 0.6873 - val_acc: 0.7940\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.2206 - acc: 0.9767 - val_loss: 0.6595 - val_acc: 0.8080\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1952 - acc: 0.9680 - val_loss: 0.7246 - val_acc: 0.8000\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.1464 - acc: 0.9827 - val_loss: 0.7817 - val_acc: 0.7800\n",
            "500/500 [==============================] - 1s 2ms/step\n",
            "Test score: 0.781665712594986\n",
            "Test accuracy: 0.7799999713897705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27-tcaborWNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "70eee50b-c091-4d3d-c8ea-a0c4caab4638"
      },
      "source": [
        "# Stacked GRU\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250,input_length=max_len))\n",
        "model.add(GRU(128,return_sequences=True))\n",
        "model.add(GRU(128,return_sequences=True))\n",
        "model.add(GRU(32, return_sequences=True))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 14s 9ms/step - loss: 1.6039 - acc: 0.2267 - val_loss: 1.5902 - val_acc: 0.2580\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 1.5394 - acc: 0.3720 - val_loss: 1.4873 - val_acc: 0.4080\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 1.2437 - acc: 0.5147 - val_loss: 1.2660 - val_acc: 0.4140\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.8963 - acc: 0.6460 - val_loss: 1.0718 - val_acc: 0.5860\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.5718 - acc: 0.8207 - val_loss: 0.9888 - val_acc: 0.6300\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.3884 - acc: 0.8880 - val_loss: 0.9026 - val_acc: 0.6900\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2607 - acc: 0.9487 - val_loss: 0.9229 - val_acc: 0.6940\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1783 - acc: 0.9880 - val_loss: 0.9045 - val_acc: 0.7200\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.1166 - acc: 0.9960 - val_loss: 0.9428 - val_acc: 0.7360\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0799 - acc: 0.9993 - val_loss: 0.9485 - val_acc: 0.7280\n",
            "500/500 [==============================] - 1s 2ms/step\n",
            "Test score: 0.9484555423259735\n",
            "Test accuracy: 0.7279999852180481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd-OuN5Ptdst",
        "colab_type": "text"
      },
      "source": [
        "The stacked GRU model has a higher accuracy score than its LSTM counterpart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyLltTMQQevc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "6afd66b3-2752-4f3d-fe3a-e0c5b0135b5d"
      },
      "source": [
        "# E Bidirectional \n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 250))\n",
        "model.add(layers.Bidirectional(layers.LSTM(250)))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 1.6025 - acc: 0.2680 - val_loss: 1.5869 - val_acc: 0.3640\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 1.4679 - acc: 0.4273 - val_loss: 1.2247 - val_acc: 0.5120\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 1.1360 - acc: 0.5787 - val_loss: 1.0706 - val_acc: 0.6560\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.8821 - acc: 0.6893 - val_loss: 0.8576 - val_acc: 0.6360\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.5817 - acc: 0.7627 - val_loss: 0.7617 - val_acc: 0.7420\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.4295 - acc: 0.8593 - val_loss: 0.7604 - val_acc: 0.7360\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.2793 - acc: 0.8933 - val_loss: 0.6221 - val_acc: 0.7780\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.1285 - acc: 0.9727 - val_loss: 0.6636 - val_acc: 0.8100\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0579 - acc: 0.9927 - val_loss: 0.6161 - val_acc: 0.8040\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0399 - acc: 0.9940 - val_loss: 0.6674 - val_acc: 0.8380\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0280 - acc: 0.9947 - val_loss: 0.7376 - val_acc: 0.8220\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 29s 20ms/step - loss: 0.0364 - acc: 0.9920 - val_loss: 0.9080 - val_acc: 0.8000\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.1295 - acc: 0.9693 - val_loss: 0.6631 - val_acc: 0.8020\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0568 - acc: 0.9793 - val_loss: 0.8405 - val_acc: 0.8200\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0353 - acc: 0.9907 - val_loss: 0.9317 - val_acc: 0.8180\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0580 - acc: 0.9900 - val_loss: 0.8462 - val_acc: 0.7900\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0913 - acc: 0.9787 - val_loss: 0.8387 - val_acc: 0.8360\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0721 - acc: 0.9853 - val_loss: 0.8300 - val_acc: 0.8380\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0382 - acc: 0.9907 - val_loss: 0.7682 - val_acc: 0.8060\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0525 - acc: 0.9873 - val_loss: 0.8835 - val_acc: 0.8320\n",
            "500/500 [==============================] - 2s 4ms/step\n",
            "Test score: 0.8835113048553467\n",
            "Test accuracy: 0.8320000171661377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIStrzhQrv1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "d211b964-835d-42db-abf6-c0926092d149"
      },
      "source": [
        "# Bidirectional GRU  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 250))\n",
        "model.add(layers.Bidirectional(layers.GRU(250)))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 1.6037 - acc: 0.2520 - val_loss: 1.5878 - val_acc: 0.2920\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.5474 - acc: 0.3980 - val_loss: 1.5524 - val_acc: 0.3480\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.4275 - acc: 0.4900 - val_loss: 1.4177 - val_acc: 0.4600\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.1055 - acc: 0.7633 - val_loss: 1.2894 - val_acc: 0.5820\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.8976 - acc: 0.8573 - val_loss: 1.0681 - val_acc: 0.6100\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.5652 - acc: 0.8933 - val_loss: 0.7468 - val_acc: 0.7960\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.2529 - acc: 0.9513 - val_loss: 0.5143 - val_acc: 0.8260\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.1972 - acc: 0.9513 - val_loss: 0.6386 - val_acc: 0.7660\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.1619 - acc: 0.9793 - val_loss: 0.6228 - val_acc: 0.7980\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0835 - acc: 0.9967 - val_loss: 0.4108 - val_acc: 0.8540\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0246 - acc: 0.9967 - val_loss: 0.4013 - val_acc: 0.8760\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0096 - acc: 0.9980 - val_loss: 0.4767 - val_acc: 0.8800\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.4459 - val_acc: 0.8740\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.3729 - val_acc: 0.8980\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3679 - val_acc: 0.9020\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 8.1832e-04 - acc: 1.0000 - val_loss: 0.3793 - val_acc: 0.8960\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 29s 19ms/step - loss: 5.2335e-04 - acc: 1.0000 - val_loss: 0.4038 - val_acc: 0.8900\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 3.5933e-04 - acc: 1.0000 - val_loss: 0.4241 - val_acc: 0.8980\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 2.5925e-04 - acc: 1.0000 - val_loss: 0.4281 - val_acc: 0.9000\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.8029e-04 - acc: 1.0000 - val_loss: 0.4356 - val_acc: 0.9000\n",
            "500/500 [==============================] - 2s 4ms/step\n",
            "Test score: 0.43564482033252716\n",
            "Test accuracy: 0.8999999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq7y6cEewE_w",
        "colab_type": "text"
      },
      "source": [
        "The GRU bidirectional model has a higher accuracy score than its LSTM counterpart. Also, it was the model with the best performance. Its accuracy was improved by using dropout, randomly setting a fraction rate of input units to a value of 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIl804HnM7w-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "b49d6eb5-515f-4afa-f0e6-2d8c2f63f7c1"
      },
      "source": [
        "#F\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250, input_length=max_len))\n",
        "model.add(LSTM(250,dropout=0.5))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(5, activation= 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 14s 9ms/step - loss: 1.6077 - acc: 0.2173 - val_loss: 1.6009 - val_acc: 0.2400\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.5855 - acc: 0.3193 - val_loss: 1.5559 - val_acc: 0.3260\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.5320 - acc: 0.3887 - val_loss: 1.3040 - val_acc: 0.4020\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.3423 - acc: 0.4713 - val_loss: 1.4209 - val_acc: 0.4780\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.2360 - acc: 0.6020 - val_loss: 1.1722 - val_acc: 0.5360\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.0589 - acc: 0.6940 - val_loss: 1.0623 - val_acc: 0.6620\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.8241 - acc: 0.7820 - val_loss: 0.8185 - val_acc: 0.6460\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.5694 - acc: 0.7660 - val_loss: 0.7729 - val_acc: 0.7340\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.4334 - acc: 0.8427 - val_loss: 0.7057 - val_acc: 0.7680\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2642 - acc: 0.9453 - val_loss: 0.5265 - val_acc: 0.8160\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.1565 - acc: 0.9600 - val_loss: 0.5713 - val_acc: 0.8100\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 16s 11ms/step - loss: 0.1012 - acc: 0.9727 - val_loss: 0.6535 - val_acc: 0.8200\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0641 - acc: 0.9860 - val_loss: 0.5931 - val_acc: 0.8360\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0401 - acc: 0.9907 - val_loss: 0.4707 - val_acc: 0.8720\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.6663 - val_acc: 0.8220\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0130 - acc: 0.9993 - val_loss: 0.5340 - val_acc: 0.8720\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.7093 - val_acc: 0.8480\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0073 - acc: 0.9973 - val_loss: 0.5526 - val_acc: 0.8680\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.5951 - val_acc: 0.8220\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 0.8700\n",
            "500/500 [==============================] - 1s 2ms/step\n",
            "Test score: 0.46069322526454926\n",
            "Test accuracy: 0.8700000047683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB3_fCsfNLBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "8e65f92f-5dab-4808-bd03-ea5c9c03d527"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 250,input_length=max_len))\n",
        "model.add(GRU(128,dropout=0.5,return_sequences=True))\n",
        "model.add(GRU(128,dropout=0.5,return_sequences=True))\n",
        "model.add(GRU(32,dropout=0.5,return_sequences=True))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 1.6083 - acc: 0.2207 - val_loss: 1.6022 - val_acc: 0.2980\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.5974 - acc: 0.2840 - val_loss: 1.5930 - val_acc: 0.2760\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.5761 - acc: 0.3040 - val_loss: 1.5660 - val_acc: 0.3500\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.4734 - acc: 0.4073 - val_loss: 1.4140 - val_acc: 0.4200\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 1.2239 - acc: 0.5213 - val_loss: 1.2586 - val_acc: 0.4800\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.9273 - acc: 0.6420 - val_loss: 1.0727 - val_acc: 0.5760\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.6513 - acc: 0.7960 - val_loss: 0.9471 - val_acc: 0.6500\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.3661 - acc: 0.9013 - val_loss: 0.8505 - val_acc: 0.6960\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.2278 - acc: 0.9473 - val_loss: 0.8233 - val_acc: 0.7440\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.1377 - acc: 0.9700 - val_loss: 0.8938 - val_acc: 0.7440\n",
            "500/500 [==============================] - 1s 2ms/step\n",
            "Test score: 0.8938008844852448\n",
            "Test accuracy: 0.7440000176429749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rihteXeWAABA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "a956c747-3aeb-424c-e67e-4e4a460061a4"
      },
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 250))\n",
        "model.add(layers.Bidirectional(layers.GRU(250, dropout=0.5)))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=250)\n",
        "\n",
        "score, acc = model.evaluate(X_val, y_val,\n",
        "                            batch_size=250)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/25\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 1.6071 - acc: 0.2140 - val_loss: 1.5964 - val_acc: 0.2980\n",
            "Epoch 2/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.5757 - acc: 0.3667 - val_loss: 1.5776 - val_acc: 0.3400\n",
            "Epoch 3/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.5199 - acc: 0.4133 - val_loss: 1.5354 - val_acc: 0.3360\n",
            "Epoch 4/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.3564 - acc: 0.5587 - val_loss: 1.1754 - val_acc: 0.5780\n",
            "Epoch 5/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 1.0518 - acc: 0.6420 - val_loss: 1.1782 - val_acc: 0.5060\n",
            "Epoch 6/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.7755 - acc: 0.7900 - val_loss: 0.8887 - val_acc: 0.7460\n",
            "Epoch 7/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.4795 - acc: 0.9120 - val_loss: 0.6447 - val_acc: 0.7600\n",
            "Epoch 8/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.2642 - acc: 0.9333 - val_loss: 0.5204 - val_acc: 0.8440\n",
            "Epoch 9/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.1360 - acc: 0.9813 - val_loss: 0.3647 - val_acc: 0.8860\n",
            "Epoch 10/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0481 - acc: 0.9867 - val_loss: 0.4144 - val_acc: 0.8760\n",
            "Epoch 11/25\n",
            "1500/1500 [==============================] - 29s 20ms/step - loss: 0.0184 - acc: 0.9987 - val_loss: 0.3664 - val_acc: 0.8900\n",
            "Epoch 12/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.3702 - val_acc: 0.8860\n",
            "Epoch 13/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.4237 - val_acc: 0.8940\n",
            "Epoch 14/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.4228 - val_acc: 0.8940\n",
            "Epoch 15/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.9000\n",
            "Epoch 16/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4137 - val_acc: 0.9080\n",
            "Epoch 17/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 8.5231e-04 - acc: 1.0000 - val_loss: 0.4242 - val_acc: 0.9100\n",
            "Epoch 18/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4323 - val_acc: 0.9040\n",
            "Epoch 19/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 7.1258e-04 - acc: 1.0000 - val_loss: 0.4551 - val_acc: 0.9000\n",
            "Epoch 20/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 5.0632e-04 - acc: 1.0000 - val_loss: 0.4914 - val_acc: 0.8960\n",
            "Epoch 21/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 5.0557e-04 - acc: 1.0000 - val_loss: 0.5130 - val_acc: 0.8980\n",
            "Epoch 22/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 5.6585e-04 - acc: 1.0000 - val_loss: 0.5261 - val_acc: 0.8940\n",
            "Epoch 23/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 3.6606e-04 - acc: 1.0000 - val_loss: 0.5176 - val_acc: 0.9020\n",
            "Epoch 24/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 4.3466e-04 - acc: 1.0000 - val_loss: 0.5129 - val_acc: 0.9020\n",
            "Epoch 25/25\n",
            "1500/1500 [==============================] - 24s 16ms/step - loss: 3.5644e-04 - acc: 1.0000 - val_loss: 0.5133 - val_acc: 0.9000\n",
            "500/500 [==============================] - 2s 4ms/step\n",
            "Test score: 0.5132521688938141\n",
            "Test accuracy: 0.8999999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2_KyVLj-MMS",
        "colab_type": "text"
      },
      "source": [
        "4) \n",
        "The bidirectional model was the best performer, because it has two hidden layers of opposite directions to the same output. Therefore, this model gets information from past and future states simultaneously. In this case, it reads the text in the news article as a cummulative to obtain the general topic. It emulates a person skimming passages to understand what they are about, without retaining most of the passages'content. The former is key to understand why the bidirectional LSTM performed worst than the bidirectional GRU; as the previosuly described task does not require the memory unit of the LSTM model. So, whether the GRU model or LSTM model is a better performer depends on the context.  \n",
        "\n",
        "The models above, specifically the top performer (GRU bidirectional model), may be improved using glove embeddings. These embeddings are pretrained on text from wikipedia entries, which would enable to accomplish the news classification task with fewer text data. Thus, saving time and making the model more efficient. A similar advantage, would occur with the combination of the GRU layers with the Conv1d. In this case, the Conv1d reduces the input vector's length as the model reaches the GRU layers. It makes the layers smaller. So, less computational power is needed when training the model. \n",
        "\n",
        "Also, the addition of the Conv1d would allow for longer sequence processing. In this case, it permits \n",
        "\n",
        "On the other hand, adding more layers and hidden nodes to this model may cause overfitting, even with the use of dropout. Thus, the optimal selection of layers and hidden nodes varies on a case by case basis. Therefore, it is uncertain whether it would have a positive or negative effect on this model. \n",
        "\n",
        "\n"
      ]
    }
  ]
}